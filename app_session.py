import streamlit as st
import pandas as pd
import numpy as np
from io import BytesIO

# --- ëŒ€ë¶„ë¥˜ ë§¤í•‘ ---
combined_mapping_dict = {
    ('ì§ì¥ ë‚´ ëŒ€ì¸ê´€ê³„', 'ìƒì‚¬ì™€ì˜ ê°ˆë“±'): 'ì§ì¥',
    ('ì§ì¥ ë‚´ ëŒ€ì¸ê´€ê³„', 'ë¶€í•˜ì™€ì˜ ê°ˆë“±'): 'ì§ì¥',
    ('ì§ì¥ ë‚´ ëŒ€ì¸ê´€ê³„', 'ë™ë£Œì™€ì˜ ê°ˆë“±'): 'ì§ì¥',
    ('ì§ì¥ ë‚´ ëŒ€ì¸ê´€ê³„', 'íŒ€ì›Œí¬ ë° ì‚¬ê¸° ì €í•˜'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì´ì§ê³ ë¯¼'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì¡°ì§ë¬¸í™” ë¶€ì ì‘'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì—…ë¬´ ìŠ¤íŠ¸ë ˆìŠ¤'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì¼í•  ì˜ìš•ì˜ ìƒì‹¤'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì§ë¬´ì¬ë°°ì¹˜(TM,ì „ê·¼,ì´ì§)'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ìŠ¹ì§„ë¬¸ì œ'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'êµëŒ€ê·¼ë¬´'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì—´ì•…í•œ ê·¼ë¬´ì¡°ê±´'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì§•ê³„/ê°ë´‰/ì§ìœ„í•´ì œ'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ë³´ìˆ˜ì— ë¶ˆë§Œ'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì‚°ì¬ ìŠ¤íŠ¸ë ˆìŠ¤'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì”ì—…/íŠ¹ê·¼'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì¶œí‡´ê·¼ ê´€ë ¨'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì ì„±ì— ë§ì§€ ì•ŠëŠ” ì—…ë¬´'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì„±ì°¨ë³„'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì„±í¬ë¡±'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ê°ì •ë…¸ë™'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'ì¦ì€ ì•¼ê·¼'): 'ì§ì¥',
    ('ì§ë¬´ìŠ¤íŠ¸ë ˆìŠ¤', 'íœ´ì§í›„ ë³µê·€'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ë¦¬ë”ì‹­'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ì‹œê°„ê´€ë¦¬'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ì§„ë¡œ ë° ê²½ë ¥ê³„ë°œ'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ì‹¤ì§ or ì •ë¦¬í•´ê³  ìŠ¤íŠ¸ë ˆìŠ¤'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ì‹¤ì ì••ë°•'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'ì •ë…„í‡´ì§'): 'ì§ì¥',
    ('ì—­ëŸ‰ ë° ê²½ë ¥', 'í‡´ì§ í›„ ì¸ìƒì„¤ê³„'): 'ì§ì¥',
    ('í•™êµìƒí™œ', 'ì „ê³µ/ì§„ë¡œ ë¬¸ì œ'): 'í•™êµ',
    ('í•™êµìƒí™œ', 'í•™ì—…/í•™ìŠµ ë¬¸ì œ'): 'í•™êµ',
    ('í•™êµìƒí™œ', 'íœ´í•™ ë° ë³µí•™'): 'í•™êµ',
    ('í•™êµìƒí™œ', 'ê²½ì œì  ë¬¸ì œ (í•™ìê¸ˆ)'): 'í•™êµ',
    ('í•™êµ ë‚´ ëŒ€ì¸ê´€ê³„', 'êµìˆ˜ì™€ì˜ ê´€ê³„'): 'í•™êµ',
    ('í•™êµ ë‚´ ëŒ€ì¸ê´€ê³„', 'ì¹œêµ¬ì™€ì˜ ê´€ê³„'): 'í•™êµ',
    ('í•™êµ ë‚´ ëŒ€ì¸ê´€ê³„', 'ì„ í›„ë°°ì™€ì˜ ê´€ê³„'): 'í•™êµ',
    ('ë¶€ë¶€', 'ë¶€ë¶€ê°ˆë“±'): 'ê°€ì¡±',
    ('ë¶€ë¶€', 'ì´í˜¼ê³ ë¯¼'): 'ê°€ì¡±',
    ('ë¶€ë¶€', 'ì‚¬ë³„'): 'ê°€ì¡±',
    ('ê°€ì¡±', 'ê°€ì¡±ê°ˆë“±(ë¶€ëª¨, í˜•ì œ ë“±)'): 'ê°€ì¡±',
    ('ê°€ì¡±', 'ê³ ë¶€/ì²˜ê°€ ê°ˆë“±'): 'ê°€ì¡±',
    ('ê°€ì¡±', 'ë…¸ë¶€ëª¨ ë´‰ì–‘'): 'ê°€ì¡±',
    ('ê°€ì¡±', 'ê°€ì •í­ë ¥'): 'ê°€ì¡±',
    ('ê°€ì¡±', 'ê°€ì¡±ì‚¬ë§(ë¶€ëª¨, í˜•ì œ ë“±)'): 'ê°€ì¡±',
    ('ìë…€', 'ê°€ì¡±ëŒë´„(ì¥ì• ë³´í˜¸, ìš”ì–‘ë³´í˜¸)'): 'ê°€ì¡±',
    ('ìë…€', 'ìë…€ì–‘ìœ¡/êµìœ¡'): 'ê°€ì¡±',
    ('ìë…€', 'ìë…€ ì •ì„œë¬¸ì œ(ìš°ìš¸, ë¶ˆì•ˆ, ADHD)'): 'ê°€ì¡±',
    ('ìë…€', 'ë°œë‹¬ì§€ì—°'): 'ê°€ì¡±',
    ('ìë…€', 'ì¥ì• ìë…€ ëŒë´„'): 'ê°€ì¡±',
    ('ì¼ë°˜', 'ëŒ€ì¸ê´€ê³„'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì‚¶ì˜ ì˜ë¯¸ì™€ ëª©í‘œ'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì„±ê²©ê³ ë¯¼'): 'ê°œì¸',
    ('ì¼ë°˜', 'ìê¸°ê³„ë°œ ë° ì„±ì¥'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì´ì„±êµì œ'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì„±ìƒí™œë¬¸ì œ'): 'ê°œì¸',
    ('ì¼ë°˜', 'ê±´ê°• ë° ì§ˆë³‘ ê´€ë ¨ ìŠ¤íŠ¸ë ˆìŠ¤'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì½”ë¡œë…¸ ê´€ë ¨ ìŠ¤íŠ¸ë ˆìŠ¤'): 'ê°œì¸',
    ('ì¼ë°˜', 'ì¢…êµì /ì˜ì  ë¬¸ì œ'): 'ê°œì¸',
    ('ì¼ë°˜', 'ë¹„ë§Œ'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ì •ì„œì†Œì§„(ë²ˆì•„ì›ƒ)'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ìš°ìš¸ì¦'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ë¶ˆì•ˆ'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ê³µí™©ì¥ì• '): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ë¶ˆë©´ì¦'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ê°•ë°•'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ì§œì¦/ë¶„ë…¸'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ê°ì •ê¸°ë³µ'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ì¡°ìš¸ì¦'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'í™˜ê°/í™˜ì²­'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'PTSD'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ìí•´'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ë¬´ê¸°ë ¥'): 'ê°œì¸',
    ('ì •ì‹ ê±´ê°•', 'ì‚°í›„ìš°ìš¸'): 'ê°œì¸',
    ('ì¤‘ë…', 'ì¸í„°ë„·/ê²Œì„/ìŠ¤ë§ˆíŠ¸í°'): 'ê°œì¸',
    ('ì¤‘ë…', 'ì•Œì½”ì˜¬'): 'ê°œì¸',
    ('ì¤‘ë…', 'ë„ë°•'): 'ê°œì¸',
    ('ì¤‘ë…', 'í¡ì—°'): 'ê°œì¸',
    ('ìì‚´ìœ„ê¸°', 'ìì‚´ìƒê°'): 'ê°œì¸',
    ('ìì‚´ìœ„ê¸°', 'ìì‚´ê³„íš'): 'ê°œì¸',
    ('ìì‚´ìœ„ê¸°', 'ìì‚´ì‹œë„ê²½í—˜'): 'ê°œì¸',
    ('ì¬ì • ë° ë²•ë¥ ìë¬¸ ë“±', 'ë²•ë¥ ìƒë‹´(ë³€í˜¸ì‚¬)'): 'ê°œì¸',
    ('ì¬ì • ë° ë²•ë¥ ìë¬¸ ë“±', 'íšŒê³„ìƒë‹´(íšŒê³„ì‚¬)'): 'ê°œì¸',
    ('ì¬ì • ë° ë²•ë¥ ìë¬¸ ë“±', 'ì¬ë¬´ìƒë‹´(ì¬ë¬´ì„¤ê³„ì‚¬)'): 'ê°œì¸',
    ('ì¬ì • ë° ë²•ë¥ ìë¬¸ ë“±', 'ì„¸ë¬´ìƒë‹´(ì„¸ë¬´ì‚¬)'): 'ê°œì¸',
    ('ì¬ì • ë° ë²•ë¥ ìë¬¸ ë“±', 'ê±´ê°•ìƒë‹´'): 'ê°œì¸',
}
def clean_str(s):
    if pd.isnull(s):
        return ""
    return str(s).strip().replace(" ", "").lower()

combined_mapping_dict_clean = {
    (clean_str(k1), clean_str(k2)): v
    for (k1, k2), v in combined_mapping_dict.items()
}

def map_region(row):
    if clean_str(row['ì£¼í˜¸ì†Œ1']) == 'ê¸°íƒ€':
        return 'ê¸°íƒ€'
    return combined_mapping_dict_clean.get(
        (clean_str(row['ì£¼í˜¸ì†Œ1']), clean_str(row['í•˜ìœ„ìš”ì†Œ1'])), None
    )
def make_area_sum_table(df, area_col, main_col, sub_col, label=""):
    temp = df.dropna(subset=[main_col, sub_col])
    area_sum = (
        temp.groupby(area_col).size().reset_index(name='ì˜ì—­ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„')
        .sort_values('ì˜ì—­ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„', ascending=False)
    )
    total_row = pd.DataFrame({
        area_col: ['í•©ê³„'],
        'ì˜ì—­ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„': [area_sum['ì˜ì—­ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„'].sum()]
    })
    area_sum_with_total = pd.concat([area_sum, total_row], ignore_index=True)
    st.markdown(f"#### {label} ì˜ì—­ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„")
    st.dataframe(area_sum_with_total)

def make_main_issue_sum_table(df, area_col, main_col, label=""):
    # ì˜ì—­ë³„ ì£¼í˜¸ì†Œë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„
    count_df = (
        df
        .dropna(subset=[area_col, main_col])
        .groupby([area_col, main_col])
        .size()
        .reset_index(name=f"{main_col}ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„")
        .sort_values([area_col, f"{main_col}ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„"], ascending=[True, False])
        .reset_index(drop=True)
    )
    # í•©ê³„ í–‰ ì¶”ê°€
    total_row = pd.DataFrame({
        area_col: ['í•©ê³„'],
        main_col: [''],
        f"{main_col}ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„": [count_df[f"{main_col}ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„"].sum()]
    })
    count_df_with_total = pd.concat([count_df, total_row], ignore_index=True)
    st.markdown(f"#### {label} {main_col}ë³„ ìƒë‹´ê±´ìˆ˜ í•©ê³„")
    st.dataframe(count_df_with_total)

st.sidebar.title("ğŸ“‹ ë©”ë‰´")
menu = st.sidebar.radio("ì´ë™í•  ì„¹ì…˜ì„ ì„ íƒí•˜ì„¸ìš”:", [
    "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° ê²°ì¸¡ì¹˜ í™•ì¸",
    "ğŸ“Š ìš´ì˜ ìš”ì•½",
    "ğŸ“ˆ ìƒë‹´ í†µê³„",
    "ğŸ§  ì‹¬ë¦¬ì§„ë‹¨ í†µê³„",
    "ğŸ—‚ï¸ ìƒë‹´ ì£¼ì œë³„ í†µê³„"
])

# --- 1. íŒŒì¼ ì—…ë¡œë“œ ë° ê²°ì¸¡ì¹˜ í™•ì¸ ---
if menu == "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° ê²°ì¸¡ì¹˜ í™•ì¸":
    st.title("ë³´ê³ ì„œìš© ë°ì´í„° ì¶”ì¶œ í”„ë¡œê·¸ë¨")
    st.markdown("---")
    st.info(
        "ì—…ë¡œë“œ ì „ ê¼­ í™•ì¸í•˜ì„¸ìš”!\n"
        "- ìƒë‹´ ì´ë ¥ ì—‘ì…€ íŒŒì¼ì—ì„œ ì•„ì´ë””ì™€ íœ´ëŒ€ì „í™”ë²ˆí˜¸ ë“± ê°œì¸ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì‚­ì œí•œ í›„ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "- í•´ë‹¹ ì—´(ì»¬ëŸ¼)ì€ ë°˜ë“œì‹œ ì‚­ì œ ë˜ëŠ” ë¹„ì‹ë³„(ë¹ˆì¹¸) ì²˜ë¦¬ í›„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    )

    col1, col2 = st.columns(2)
    with col1:
        uploaded_counseling = st.file_uploader("ìƒë‹´ ì´ë ¥ ì—‘ì…€ íŒŒì¼", type=["xlsx"], key="counseling")
    with col2:
        uploaded_diagnosis = st.file_uploader("ì§„ë‹¨ ì´ë ¥ ì—‘ì…€ íŒŒì¼", type=["xlsx"], key="diagnosis")

    if uploaded_counseling is not None:
        df_counseling = pd.read_excel(uploaded_counseling)
        st.write("ì¹¼ëŸ¼ëª…:", df_counseling.columns.tolist())   # â† ì´ ì¤„ ì¶”ê°€
        df_counseling.columns = df_counseling.columns.str.strip()
        df_counseling['ì˜ì—­1'] = df_counseling.apply(
            lambda row: map_region({'ì£¼í˜¸ì†Œ1': row['ì£¼í˜¸ì†Œ1'], 'í•˜ìœ„ìš”ì†Œ1': row['í•˜ìœ„ìš”ì†Œ1']}), axis=1)
        df_counseling['ì˜ì—­2'] = df_counseling.apply(
            lambda row: map_region({'ì£¼í˜¸ì†Œ1': row['ì£¼í˜¸ì†Œ2'], 'í•˜ìœ„ìš”ì†Œ1': row['í•˜ìœ„ìš”ì†Œ2']}), axis=1)
        df_counseling['ì˜ì—­3'] = df_counseling.apply(
            lambda row: map_region({'ì£¼í˜¸ì†Œ1': row['ì£¼í˜¸ì†Œ3'], 'í•˜ìœ„ìš”ì†Œ1': row['í•˜ìœ„ìš”ì†Œ3']}), axis=1)
        st.session_state['df_counseling'] = df_counseling

        # ê°œì¸ì •ë³´ ì—´ í™•ì¸
        sensitive_columns = ['ì‹ ì²­ì§ì›ì´ë¦„', 'íœ´ëŒ€í°ë²ˆí˜¸']
        sensitive_found = [col for col in sensitive_columns if col in df_counseling.columns]
        if sensitive_found:
            st.error(
                f"ì—…ë¡œë“œ íŒŒì¼ì— ê°œì¸ì •ë³´ ì—´({', '.join(sensitive_found)})ì´ í¬í•¨ë˜ì–´ ìˆì–´ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n\n"
                "í•´ë‹¹ ì—´ì„ ì‚­ì œí•œ í›„ ë‹¤ì‹œ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
            )
            st.stop()

        # ì§„ë‹¨ ë°ì´í„°
        if uploaded_diagnosis is not None:
            df_diagnosis = pd.read_excel(uploaded_diagnosis)
            df_diagnosis.columns = df_diagnosis.columns.str.strip()
        else:
            df_diagnosis = pd.DataFrame(columns=['ì•„ì´ë””', 'ì§„ë‹¨ì‹¤ì‹œì¼', 'ì§„ë‹¨ì—°ì›”', 'ì§„ë‹¨ëª…', 'ì‹œí–‰ë²ˆí˜¸'])
            st.warning("âš ï¸ ì§„ë‹¨ ì´ë ¥ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒë‹´ ì´ë ¥ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

        # ë‚ ì§œ, ì—°ì›”, ì—°ë ¹ëŒ€, ì„±ë³„ ë“± ì „ì²˜ë¦¬
        df_counseling['ìƒë‹´ì‹¤ì‹œì¼'] = pd.to_datetime(df_counseling['ìƒë‹´ì‹¤ì‹œì¼'], errors='coerce')
        df_counseling['ìƒë‹´ì—°ì›”'] = df_counseling['ìƒë‹´ì‹¤ì‹œì¼'].dt.to_period('M').astype(str)
        df_counseling['ì—°ë ¹ëŒ€'] = (df_counseling['ì‹ ì²­ì§ì›ë‚˜ì´'] // 10 * 10).astype('Int64').astype(str) + 'ëŒ€'
        df_counseling['ì„±ë³„'] = df_counseling['ì‹ ì²­ì§ì›ì„±ë³„'].fillna('ë¯¸ìƒ')

        if len(df_diagnosis) > 0:
            df_diagnosis['ì§„ë‹¨ì‹¤ì‹œì¼'] = pd.to_datetime(df_diagnosis['ì§„ë‹¨ì‹¤ì‹œì¼'], errors='coerce')
            df_diagnosis['ì§„ë‹¨ì—°ì›”'] = df_diagnosis['ì§„ë‹¨ì‹¤ì‹œì¼'].dt.to_period('M').astype(str)

        # ê²°ì¸¡ì¹˜ ìš”ì•½
        def missing_summary(df, name):
            summary = pd.DataFrame({
                'ê²°ì¸¡ì¹˜ ìˆ˜': df.isnull().sum(),
                'ì „ì²´ í–‰ ìˆ˜': len(df)
            })
            summary['ê²°ì¸¡ë¥ (%)'] = (summary['ê²°ì¸¡ì¹˜ ìˆ˜'] / summary['ì „ì²´ í–‰ ìˆ˜'] * 100).round(1)
            summary = summary[summary['ê²°ì¸¡ì¹˜ ìˆ˜'] > 0]
            if summary.empty:
                st.success(f"{name} ë°ì´í„°ì— ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning(f"{name} ë°ì´í„°ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.")
                st.dataframe(summary)

        st.markdown("---")
        st.header("ì „ì²´ ê²°ì¸¡ì¹˜ ìš”ì•½í‘œ")
        col1, col2 = st.columns(2)
        with col1:
            with st.expander("ìƒë‹´ ì´ë ¥ ê²°ì¸¡ì¹˜"):
                missing_summary(df_counseling, "ìƒë‹´ ì´ë ¥")
        with col2:
            with st.expander("ì§„ë‹¨ ì´ë ¥ ê²°ì¸¡ì¹˜"):
                missing_summary(df_diagnosis, "ì§„ë‹¨ ì´ë ¥")

        # ì›” ëª©ë¡ êµ¬í•˜ê¸°
        valid_months_counseling = df_counseling['ìƒë‹´ì‹¤ì‹œì¼'].dropna()
        valid_months_diagnosis = df_diagnosis['ì§„ë‹¨ì‹¤ì‹œì¼'].dropna()
        all_valid_months = pd.concat([valid_months_counseling, valid_months_diagnosis])

        if len(all_valid_months) > 0:
            min_month = all_valid_months.min().to_period('M')
            max_month = all_valid_months.max().to_period('M')
            all_months = pd.period_range(min_month, max_month, freq='M').astype(str).tolist()
        else:
            all_months = []

        st.session_state['df_counseling'] = df_counseling
        st.session_state['df_diagnosis'] = df_diagnosis
        st.session_state['all_months'] = all_months

        # ID í´ë¦¬ë‹
        def clean_id(val):
            if pd.isnull(val):
                return None
            val = str(val).strip().replace(' ', '').replace('\u3000', '').lower()
            if val.endswith('.0'):
                val = val[:-2]
            if not val or val in {'nan', 'none', '0', '0.0'}:
                return None
            return val

        counseling_ids = df_counseling['ì•„ì´ë””'].apply(clean_id)
        diagnosis_ids = df_diagnosis['ì•„ì´ë””'].apply(clean_id)
        combined_ids = pd.concat([counseling_ids, diagnosis_ids])
        unique_ids = combined_ids.dropna().drop_duplicates()
        ì‹¤ê³„_ì¸ì›ìˆ˜ = len(unique_ids)

        # ì„¸ì…˜ì— ì €ì¥ (ë‹¤ìŒ ë©”ë‰´ì—ì„œ ì‚¬ìš©)
        st.session_state['df_counseling'] = df_counseling
        st.session_state['df_diagnosis'] = df_diagnosis
        st.session_state['ì‹¤ê³„_ì¸ì›ìˆ˜'] = ì‹¤ê³„_ì¸ì›ìˆ˜
        st.session_state['all_months'] = all_months

        with st.expander("ì‹¤ì œ ì¸ì›(ì¤‘ë³µ ì œê±°) ëª©ë¡"):
            st.write(unique_ids.tolist())
            st.write(f"ì‹¤ì œ ì¸ì› ìˆ˜: {ì‹¤ê³„_ì¸ì›ìˆ˜} ëª…")

# --- 2. ìš´ì˜ ìš”ì•½ ---
elif menu == "ğŸ“Š ìš´ì˜ ìš”ì•½":
    if 'df_counseling' not in st.session_state or 'df_diagnosis' not in st.session_state:
        st.warning("ë¨¼ì € 'íŒŒì¼ ì—…ë¡œë“œ'ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()
    df_counseling = st.session_state['df_counseling']
    df_diagnosis = st.session_state['df_diagnosis']
    ì‹¤ê³„_ì¸ì›ìˆ˜ = st.session_state['ì‹¤ê³„_ì¸ì›ìˆ˜']
    all_months = st.session_state['all_months']

    st.header("ğŸ“Š ìš´ì˜ ìš”ì•½")
    summary = pd.DataFrame({'ì—°ì›”': all_months})
    summary['ì‹¬ë¦¬ìƒë‹´'] = summary['ì—°ì›”'].apply(lambda m: df_counseling[df_counseling['ìƒë‹´ì—°ì›”']==m]['ì•„ì´ë””'].nunique())
    summary['ì‹¬ë¦¬ì§„ë‹¨'] = summary['ì—°ì›”'].apply(lambda m: df_diagnosis[df_diagnosis['ì§„ë‹¨ì—°ì›”']==m]['ì•„ì´ë””'].nunique())
    summary['í•©ê³„'] = summary['ì‹¬ë¦¬ìƒë‹´'] + summary['ì‹¬ë¦¬ì§„ë‹¨']
    summary.loc[len(summary)] = ['ëˆ„ê³„', summary['ì‹¬ë¦¬ìƒë‹´'].sum(), summary['ì‹¬ë¦¬ì§„ë‹¨'].sum(), summary['í•©ê³„'].sum()]
    summary.loc[len(summary)] = ['ì‹¤ê³„', df_counseling['ì•„ì´ë””'].nunique(), df_diagnosis['ì•„ì´ë””'].nunique(), ì‹¤ê³„_ì¸ì›ìˆ˜]
    st.subheader("ì„œë¹„ìŠ¤ ì´ìš© ì¸ì›")
    st.dataframe(summary, use_container_width=True)

    # ì´ìš© íšŸìˆ˜ ìš”ì•½
    summary_count = pd.DataFrame({'ì—°ì›”': all_months})
    summary_count['ì‹¬ë¦¬ìƒë‹´'] = summary_count['ì—°ì›”'].apply(lambda m: len(df_counseling[df_counseling['ìƒë‹´ì—°ì›”']==m]))
    summary_count['ì‹¬ë¦¬ì§„ë‹¨'] = summary_count['ì—°ì›”'].apply(lambda m: len(df_diagnosis[df_diagnosis['ì§„ë‹¨ì—°ì›”']==m]))
    summary_count['í•©ê³„'] = summary_count['ì‹¬ë¦¬ìƒë‹´'] + summary_count['ì‹¬ë¦¬ì§„ë‹¨']
    summary_count.loc[len(summary_count)] = ['ëˆ„ê³„', summary_count['ì‹¬ë¦¬ìƒë‹´'].sum(), summary_count['ì‹¬ë¦¬ì§„ë‹¨'].sum(), summary_count['í•©ê³„'].sum()]
    st.subheader("ì„œë¹„ìŠ¤ ì´ìš© íšŸìˆ˜")
    st.dataframe(summary_count, use_container_width=True)

# --- 3. ìƒë‹´ í†µê³„ ---
elif menu == "ğŸ“ˆ ìƒë‹´ í†µê³„":
    if 'df_counseling' not in st.session_state or 'all_months' not in st.session_state:
        st.warning("ë¨¼ì € 'íŒŒì¼ ì—…ë¡œë“œ'ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()
    df_counseling = st.session_state['df_counseling']
    all_months = st.session_state['all_months']

    st.header("ğŸ“ˆ ìƒë‹´ í†µê³„")
    # ìƒë‹´ìœ í˜•ë³„ ì¸ì› ë° íšŸìˆ˜
    def clean_id(val):
        if pd.isnull(val):
            return None
        val = str(val).strip().replace(' ', '').replace('\u3000', '').lower()
        if val.endswith('.0'):
            val = val[:-2]
        if not val or val in {'nan', 'none', '0', '0.0'}:
            return None
        return val

    ìƒë‹´_ids_only = df_counseling['ì•„ì´ë””'].apply(clean_id)
    ìƒë‹´_unique_ids = ìƒë‹´_ids_only.dropna().drop_duplicates()
    ìƒë‹´_ì‹¤ê³„_ì¸ì›ìˆ˜ = len(ìƒë‹´_unique_ids)

    real_type_people = df_counseling.groupby('ìƒë‹´ìœ í˜•')['ì•„ì´ë””'].apply(
        lambda x: x.apply(clean_id).dropna().drop_duplicates().nunique()
    )

    st.subheader("1) ìƒë‹´ìœ í˜•ë³„ ì¸ì› ë° íšŸìˆ˜")
    type_people = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ìƒë‹´ìœ í˜•'])['ì•„ì´ë””'].nunique().reset_index()
    type_people_summary = type_people.pivot(index='ìƒë‹´ì—°ì›”', columns='ìƒë‹´ìœ í˜•', values='ì•„ì´ë””')
    type_people_summary = type_people_summary.reindex(all_months).fillna(0).astype(int)
    
    real_monthly_people = (
        df_counseling.dropna(subset=['ìƒë‹´ì—°ì›”', 'ì•„ì´ë””'])
        .groupby('ìƒë‹´ì—°ì›”')['ì•„ì´ë””'].nunique()
        .reindex(all_months).fillna(0).astype(int) 
    )

    type_people_summary['í•©ê³„'] = type_people_summary.index.map(real_monthly_people).fillna(0).astype(int)
    type_people_summary.loc['ëˆ„ê³„'] = type_people_summary.sum()

    ì‹¤ê³„_í–‰ = real_type_people.reindex(type_people_summary.columns[:-1]).fillna(0).astype(int).tolist()
    ì‹¤ê³„_í–‰.append(ìƒë‹´_ì‹¤ê³„_ì¸ì›ìˆ˜)
    type_people_summary.loc['ì‹¤ê³„'] = ì‹¤ê³„_í–‰

    st.markdown("ìƒë‹´ìœ í˜•ë³„ ì´ìš© ì¸ì›")
    st.dataframe(type_people_summary)

    type_counts = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ìƒë‹´ìœ í˜•'])['ì‚¬ë¡€ë²ˆí˜¸'].count().reset_index()
    type_counts_summary = type_counts.pivot(index='ìƒë‹´ì—°ì›”', columns='ìƒë‹´ìœ í˜•', values='ì‚¬ë¡€ë²ˆí˜¸')
    type_counts_summary = type_counts_summary.reindex(all_months).fillna(0).astype(int)
    type_counts_summary['í•©ê³„'] = type_counts_summary.sum(axis=1)
    type_counts_summary.loc['ëˆ„ê³„'] = type_counts_summary.sum()
    df_counseling.columns = df_counseling.columns.str.strip().str.lower()

    # 2. type_counts_summary ë§Œë“¤ê¸° (index=ìƒë‹´ì—°ì›”, columns=ìƒë‹´ìœ í˜•)
    type_counts = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ìƒë‹´ìœ í˜•'])['ì‚¬ë¡€ë²ˆí˜¸'].count().reset_index()
    type_counts_summary = type_counts.pivot(index='ìƒë‹´ì—°ì›”', columns='ìƒë‹´ìœ í˜•', values='ì‚¬ë¡€ë²ˆí˜¸')
    type_counts_summary = type_counts_summary.reindex(all_months).fillna(0).astype(int)
    type_counts_summary['í•©ê³„'] = type_counts_summary.sum(axis=1)
    type_counts_summary.loc['ëˆ„ê³„'] = type_counts_summary.sum()

    # 3. No-show ì»¬ëŸ¼ ì¶”ê°€
    no_show_col = next((col for col in df_counseling.columns if 'no-show' in col), None)
    if no_show_col:
        # print(no_show_col, df_counseling[no_show_col].unique())  # â† ì‹¤ì œ ê°’ ì ê²€ìš©
        no_show_y = (
            df_counseling[df_counseling[no_show_col].astype(str).str.upper() == 'Y']
            .groupby('ìƒë‹´ì—°ì›”')
            .size()
            .reindex(all_months).fillna(0).astype(int)
        )
        type_counts_summary['No-show(Y)'] = no_show_y
        type_counts_summary.loc['ëˆ„ê³„', 'No-show(Y)'] = no_show_y.sum()
    else:
        type_counts_summary['No-show(Y)'] = 0
        type_counts_summary.loc['ëˆ„ê³„', 'No-show(Y)'] = 0

    st.markdown("ìƒë‹´ìœ í˜•ë³„ ì´ìš© íšŸìˆ˜")
    st.dataframe(type_counts_summary)

    # 5. ë…¸ì‡¼ ê²½ê³ ë¬¸
    if no_show_col and no_show_y.sum() > 0:
        st.warning("ë…¸ì‡¼ ê°’ì´ ìˆìŠµë‹ˆë‹¤. ë…¸ì‡¼ë¥¼ ë³„ë„ë¡œ ë‹¤ë£¨ì‹œëŠ” ê²½ìš°ë¼ë©´ ì› ë°ì´í„°ì—ì„œ No-showë¡œ ì²˜ë¦¬ëœ ìœ í˜•ì„ ë‹¤ì‹œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")

    # --- ì‹ ì²­ìœ í˜•ë³„ ì¸ì› ë° íšŸìˆ˜ ---
    st.markdown("---")
    st.subheader("2) ì‹ ì²­ìœ í˜•ë³„ ì¸ì› ë° íšŸìˆ˜")

    # (1) ì‹ ì²­ìœ í˜•ë³„ ì›”ë³„ ì´ìš© ì¸ì› (ì¤‘ë³µì œê±°)
    ì‹ ì²­ìœ í˜•_people = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ìœ í˜•', 'ì•„ì´ë””'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ìœ í˜•'])['ì•„ì´ë””']
        .nunique()
        .reset_index()
    )
    ì‹ ì²­ìœ í˜•_people_summary = ì‹ ì²­ìœ í˜•_people.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ìœ í˜•', values='ì•„ì´ë””')
    ì‹ ì²­ìœ í˜•_people_summary = ì‹ ì²­ìœ í˜•_people_summary.reindex(all_months).fillna(0).astype(int)
    ì‹ ì²­ìœ í˜•_people_summary['í•©ê³„'] = ì‹ ì²­ìœ í˜•_people_summary.sum(axis=1)
    ì‹ ì²­ìœ í˜•_people_summary.loc['ëˆ„ê³„'] = ì‹ ì²­ìœ í˜•_people_summary.sum()

    #### â˜… ì—¬ê¸°ì„œë¶€í„° 'ì‹¤ê³„' í–‰ ì¶”ê°€ ì½”ë“œ
    # clean_id í•¨ìˆ˜ ì •ì˜
    def clean_id(val):
        if pd.isnull(val):
            return None
        val = str(val).strip().replace(' ', '').replace('\u3000', '').lower()
        if val.endswith('.0'):
            val = val[:-2]
        if not val or val in {'nan', 'none', '0', '0.0'}:
            return None
        return val

    # ì‹ ì²­ìœ í˜•ë³„ ì‹¤ì œ ì¸ì› ê³„ì‚°
    real_by_applytype = (
        df_counseling.dropna(subset=['ì‹ ì²­ìœ í˜•', 'ì•„ì´ë””'])
        .assign(ì•„ì´ë””=lambda x: x['ì•„ì´ë””'].apply(clean_id))
        .dropna(subset=['ì•„ì´ë””'])
        .drop_duplicates(['ì‹ ì²­ìœ í˜•', 'ì•„ì´ë””'])
        .groupby('ì‹ ì²­ìœ í˜•')['ì•„ì´ë””'].nunique()
    )
    real_apply_people_total = (
        df_counseling['ì•„ì´ë””'].apply(clean_id).dropna().drop_duplicates().shape[0]
    )

    # 'ì‹¤ê³„' í–‰(ë”•ì…”ë„ˆë¦¬ë¡œ)
    row_dict = {col: int(real_by_applytype[col]) if col in real_by_applytype and pd.notnull(real_by_applytype[col]) else 0 for col in ì‹ ì²­ìœ í˜•_people_summary.columns}
    row_dict['í•©ê³„'] = real_apply_people_total
    ì‹ ì²­ìœ í˜•_people_summary.loc['ì‹¤ê³„'] = row_dict
    ####

    st.markdown("ì‹ ì²­ìœ í˜•ë³„ ì´ìš© ì¸ì›")
    st.dataframe(ì‹ ì²­ìœ í˜•_people_summary)

    # (2) ì‹ ì²­ìœ í˜•ë³„ ì›”ë³„ ì´ìš© íšŸìˆ˜ (ìƒë‹´ ê±´ìˆ˜)
    ì‹ ì²­ìœ í˜•_counts = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ìœ í˜•'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ìœ í˜•'])['ì‚¬ë¡€ë²ˆí˜¸']
        .count()
        .reset_index()
    )
    ì‹ ì²­ìœ í˜•_counts_summary = ì‹ ì²­ìœ í˜•_counts.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ìœ í˜•', values='ì‚¬ë¡€ë²ˆí˜¸')
    ì‹ ì²­ìœ í˜•_counts_summary = ì‹ ì²­ìœ í˜•_counts_summary.reindex(all_months).fillna(0).astype(int)
    ì‹ ì²­ìœ í˜•_counts_summary['í•©ê³„'] = ì‹ ì²­ìœ í˜•_counts_summary.sum(axis=1)
    ì‹ ì²­ìœ í˜•_counts_summary.loc['ëˆ„ê³„'] = ì‹ ì²­ìœ í˜•_counts_summary.sum()
    st.markdown("ì‹ ì²­ìœ í˜•ë³„ ì´ìš© íšŸìˆ˜")
    st.dataframe(ì‹ ì²­ìœ í˜•_counts_summary)

    # ì„±ë³„ ì´ìš© ì¸ì› ë° íšŸìˆ˜
    st.markdown("---")
    st.subheader("3) ì„±ë³„ ì´ìš© ì¸ì› ë° íšŸìˆ˜")
    gender_people = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ì„±ë³„'])['ì•„ì´ë””'].nunique().reset_index()
    gender_people_summary = gender_people.pivot(index='ìƒë‹´ì—°ì›”', columns='ì„±ë³„', values='ì•„ì´ë””')
    gender_people_summary = gender_people_summary.reindex(all_months).fillna(0).astype(int)
    gender_people_summary['í•©ê³„'] = gender_people_summary.sum(axis=1)
    gender_people_summary.loc['ëˆ„ê³„'] = gender_people_summary.sum()

    # (1) ì•„ì´ë””ë³„ ì„±ë³„ ì •ë³´ ì¶”ì¶œ
    id_gender_df = df_counseling[['ì•„ì´ë””', 'ì„±ë³„']].drop_duplicates()
    id_gender_df['ì•„ì´ë””'] = id_gender_df['ì•„ì´ë””'].apply(clean_id)

    # ì•„ì´ë”” ê°’ì´ None/ë¹ˆì¹¸ì¸ í–‰ ì œì™¸
    id_gender_df = id_gender_df[~id_gender_df['ì•„ì´ë””'].isnull() & (id_gender_df['ì•„ì´ë””'] != '')]

    # ì‹¤ê³„ ì•„ì´ë”” (ì¤‘ë³µì œê±°)
    unique_ids = id_gender_df['ì•„ì´ë””'].drop_duplicates()
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(unique_ids)

    # ì„±ë³„ë³„ ì‹¤ê³„ ì¸ì› êµ¬í•˜ê¸°
    ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜ = id_gender_df['ì„±ë³„'].value_counts()
    ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜ = ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜.reindex(gender_people_summary.columns[:-1], fill_value=0)

    # í‘œ ë§¨ ì•„ë˜ 'ì‹¤ê³„' í–‰ ì¶”ê°€
    id_gender_df = df_counseling[['ì•„ì´ë””', 'ì„±ë³„']].drop_duplicates()
    id_gender_df['ì•„ì´ë””'] = id_gender_df['ì•„ì´ë””'].apply(clean_id)
    id_gender_df = id_gender_df[~id_gender_df['ì•„ì´ë””'].isnull() & (id_gender_df['ì•„ì´ë””'] != '')]
    unique_ids = id_gender_df['ì•„ì´ë””'].drop_duplicates()
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(unique_ids)
    ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜ = id_gender_df['ì„±ë³„'].value_counts()
    ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜ = ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜.reindex(gender_people_summary.columns[:-1], fill_value=0)

    # â˜… [ìˆ˜ì • í¬ì¸íŠ¸]
    ì‹¤ê³„_í–‰ = ì‹¤ê³„_ì„±ë³„_ì¸ì›ìˆ˜.tolist()
    ì‹¤ê³„_í–‰.append(ì‹¤ê³„_ì¸ì›ìˆ˜)  # ë§ˆì§€ë§‰ì— ì‹¤ì œ ì¸ì›ìˆ˜ í• ë‹¹!
    gender_people_summary.loc['ì‹¤ê³„'] = ì‹¤ê³„_í–‰

    st.markdown("ì„±ë³„ ì´ìš© ì¸ì›")
    if 'ë¯¸ìƒ' in gender_people_summary.columns:
        gender_people_summary = gender_people_summary.drop(columns='ë¯¸ìƒ')
    st.dataframe(gender_people_summary)


    gender_counts = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ì„±ë³„'])['ì‚¬ë¡€ë²ˆí˜¸'].count().reset_index()
    gender_counts_summary = gender_counts.pivot(index='ìƒë‹´ì—°ì›”', columns='ì„±ë³„', values='ì‚¬ë¡€ë²ˆí˜¸')
    gender_counts_summary = gender_counts_summary.reindex(all_months).fillna(0).astype(int)
    gender_counts_summary['í•©ê³„'] = gender_counts_summary.sum(axis=1)
    gender_counts_summary.loc['ëˆ„ê³„'] = gender_counts_summary.sum()
    st.markdown("ì„±ë³„ ì´ìš© íšŸìˆ˜")
    if 'ë¯¸ìƒ' in gender_counts_summary.columns:
        gender_counts_summary = gender_counts_summary.drop(columns='ë¯¸ìƒ')
    st.dataframe(gender_counts_summary)

    # ì—°ë ¹ë³„ ì¸ì› ë° íšŸìˆ˜
    st.markdown("---")
    st.subheader("4) ì—°ë ¹ë³„ ì¸ì› ë° íšŸìˆ˜")
    age_monthly = df_counseling.groupby(['ìƒë‹´ì—°ì›”', 'ì—°ë ¹ëŒ€']).agg(
        ì¸ì›ìˆ˜=('ì•„ì´ë””', 'nunique'),
        ê±´ìˆ˜=('ì‚¬ë¡€ë²ˆí˜¸', 'count')
    ).reset_index()
    age_pivot_people = age_monthly.pivot(index='ìƒë‹´ì—°ì›”', columns='ì—°ë ¹ëŒ€', values='ì¸ì›ìˆ˜')
    age_pivot_people = age_pivot_people.reindex(all_months).fillna(0).astype(int)
    age_pivot_people['í•©ê³„'] = age_pivot_people.sum(axis=1)

    age_pivot_cases = age_monthly.pivot(index='ìƒë‹´ì—°ì›”', columns='ì—°ë ¹ëŒ€', values='ê±´ìˆ˜')
    age_pivot_cases = age_pivot_cases.reindex(all_months).fillna(0).astype(int) 
    age_pivot_cases['í•©ê³„'] = age_pivot_cases.sum(axis=1)
    age_pivot_cases = age_pivot_cases[age_pivot_cases.index != 'NaT']
    age_pivot_cases.loc['í•©ê³„'] = age_pivot_cases.sum()
    age_pivot_people.loc['ëˆ„ê³„'] = age_pivot_people.sum()

    id_age_df = df_counseling[['ì•„ì´ë””', 'ì—°ë ¹ëŒ€']].drop_duplicates()
    id_age_df['ì•„ì´ë””'] = id_age_df['ì•„ì´ë””'].apply(clean_id)
    id_age_df = id_age_df[~id_age_df['ì•„ì´ë””'].isnull() & (id_age_df['ì•„ì´ë””'] != '')]

    # ì‹¤ê³„ ì•„ì´ë””(ì¤‘ë³µì œê±°)
    unique_ids = id_age_df['ì•„ì´ë””'].drop_duplicates()
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(unique_ids)

    last_age = (
        df_counseling
        .sort_values(['ì•„ì´ë””', 'ìƒë‹´ì‹¤ì‹œì¼'])
        .groupby('ì•„ì´ë””')
        .tail(1)[['ì•„ì´ë””','ì—°ë ¹ëŒ€']]
    )

    # ì—°ë ¹ëŒ€ë³„ ì‹¤ê³„ ì¸ì› êµ¬í•˜ê¸°
    ì‹¤ê³„_ì—°ë ¹ë³„_ì¸ì›ìˆ˜ = last_age['ì—°ë ¹ëŒ€'].value_counts().reindex(age_pivot_people.columns[:-1], fill_value=0)
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(last_age)

    # ì‹¤ê³„ í–‰ ì¶”ê°€
    id_age_df = df_counseling[['ì•„ì´ë””', 'ì—°ë ¹ëŒ€']].drop_duplicates()
    id_age_df['ì•„ì´ë””'] = id_age_df['ì•„ì´ë””'].apply(clean_id)
    id_age_df = id_age_df[~id_age_df['ì•„ì´ë””'].isnull() & (id_age_df['ì•„ì´ë””'] != '')]
    unique_ids = id_age_df['ì•„ì´ë””'].drop_duplicates()
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(unique_ids)

    last_age = (
        df_counseling
        .sort_values(['ì•„ì´ë””', 'ìƒë‹´ì‹¤ì‹œì¼'])
        .groupby('ì•„ì´ë””')
        .tail(1)[['ì•„ì´ë””', 'ì—°ë ¹ëŒ€']]
    )
    ì‹¤ê³„_ì—°ë ¹ë³„_ì¸ì›ìˆ˜ = last_age['ì—°ë ¹ëŒ€'].value_counts().reindex(age_pivot_people.columns[:-1], fill_value=0)
    ì‹¤ê³„_ì¸ì›ìˆ˜ = len(last_age)

    # â˜… [ìˆ˜ì • í¬ì¸íŠ¸]
    ì‹¤ê³„_í–‰ = ì‹¤ê³„_ì—°ë ¹ë³„_ì¸ì›ìˆ˜.tolist()
    ì‹¤ê³„_í–‰.append(ì‹¤ê³„_ì¸ì›ìˆ˜)  # ë§ˆì§€ë§‰ì— ì‹¤ì œ ì¸ì›ìˆ˜ í• ë‹¹!
    age_pivot_people.loc['ì‹¤ê³„'] = ì‹¤ê³„_í–‰

    st.markdown("ì—°ë ¹ë³„ ì´ìš© ì¸ì›")
    st.dataframe(age_pivot_people)
    st.markdown("ì—°ë ¹ë³„ ì´ìš© íšŸìˆ˜")
    st.dataframe(age_pivot_cases)

    st.markdown("---")
    st.subheader("5) ì†Œì†ë³„ ì¸ì› ë° íšŸìˆ˜")
    # (1) ì†Œì†ë³„ ì›”ë³„ ì´ìš© ì¸ì› (ì¤‘ë³µì œê±°)
    ì†Œì†_people = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì†Œì†', 'ì•„ì´ë””'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì†Œì†'])['ì•„ì´ë””']
        .nunique()
        .reset_index()
    )
    ì†Œì†_people_summary = ì†Œì†_people.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ì§ì›ì†Œì†', values='ì•„ì´ë””')
    ì†Œì†_people_summary = ì†Œì†_people_summary.reindex(all_months).fillna(0).astype(int)
    ì†Œì†_people_summary['í•©ê³„'] = ì†Œì†_people_summary.sum(axis=1)
    ì†Œì†_people_summary.loc['ëˆ„ê³„'] = ì†Œì†_people_summary.sum()

    # â˜… ì‹¤ê³„ í–‰ ì¶”ê°€
    real_by_affiliation = (
        df_counseling.dropna(subset=['ì‹ ì²­ì§ì›ì†Œì†', 'ì•„ì´ë””'])
        .assign(ì•„ì´ë””=lambda x: x['ì•„ì´ë””'].apply(clean_id))
        .dropna(subset=['ì•„ì´ë””'])
        .drop_duplicates(['ì‹ ì²­ì§ì›ì†Œì†', 'ì•„ì´ë””'])
        .groupby('ì‹ ì²­ì§ì›ì†Œì†')['ì•„ì´ë””'].nunique()
    )
    real_affiliation_total = (
        df_counseling['ì•„ì´ë””'].apply(clean_id).dropna().drop_duplicates().shape[0]
    )
    row_dict = {col: int(real_by_affiliation[col]) if col in real_by_affiliation and pd.notnull(real_by_affiliation[col]) else 0 for col in ì†Œì†_people_summary.columns}
    row_dict['í•©ê³„'] = real_affiliation_total
    ì†Œì†_people_summary.loc['ì‹¤ê³„'] = row_dict

    st.markdown("ì†Œì†ë³„ ì´ìš© ì¸ì›")
    st.dataframe(ì†Œì†_people_summary)

    # (2) ì†Œì†ë³„ ì›”ë³„ ì´ìš© íšŸìˆ˜ (ìƒë‹´ ê±´ìˆ˜)
    ì†Œì†_counts = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì†Œì†'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì†Œì†'])['ì‚¬ë¡€ë²ˆí˜¸']
        .count()
        .reset_index()
    )
    ì†Œì†_counts_summary = ì†Œì†_counts.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ì§ì›ì†Œì†', values='ì‚¬ë¡€ë²ˆí˜¸')
    ì†Œì†_counts_summary = ì†Œì†_counts_summary.reindex(all_months).fillna(0).astype(int)
    ì†Œì†_counts_summary['í•©ê³„'] = ì†Œì†_counts_summary.sum(axis=1)
    ì†Œì†_counts_summary.loc['ëˆ„ê³„'] = ì†Œì†_counts_summary.sum()

    st.markdown("ì†Œì†ë³„ ì´ìš© íšŸìˆ˜")
    st.dataframe(ì†Œì†_counts_summary)

    st.markdown("---")
    st.subheader("6) ì§ê¸‰ë³„ ì¸ì› ë° íšŸìˆ˜")
 
    # 1. ì•„ì´ë”” ì •ì œ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì, ë°ì´í„°íƒ€ì… í†µì¼)
    df_counseling['ì•„ì´ë””_ì •ì œ'] = (
        df_counseling['ì•„ì´ë””']
        .astype(str)
        .str.replace(r'\s+', '', regex=True)   # ëª¨ë“  ê³µë°± ì œê±°
        .str.replace('\u3000', '')             # ì „ê° ê³µë°±ë„ ì œê±°
        .str.strip()
    )

    # 2. ì§ê¸‰ ê³µë€/NaNì„ 'ë¯¸ìƒ'ìœ¼ë¡œ í†µì¼
    df_counseling['ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬'] = df_counseling['ì‹ ì²­ì§ì›ì§ë¬´'].fillna('ë¯¸ìƒ').replace('', 'ë¯¸ìƒ')

    # ìƒë‹´ì—°ì›” NaT/ê²°ì¸¡/ë¹ˆê°’ ì œê±°
    df_counseling = df_counseling[
        df_counseling['ìƒë‹´ì—°ì›”'].notna() &
        (df_counseling['ìƒë‹´ì—°ì›”'].astype(str) != 'NaT') &
        (df_counseling['ìƒë‹´ì—°ì›”'].astype(str) != 'nan') &
        (df_counseling['ìƒë‹´ì—°ì›”'].astype(str) != '')
    ]

    # 3. ì›”ë³„ ì§ê¸‰ë³„ ì¤‘ë³µ ì—†ëŠ” ì¸ì›ìˆ˜
    duty_people = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì•„ì´ë””_ì •ì œ'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬'])['ì•„ì´ë””_ì •ì œ']
        .nunique()
        .reset_index()
    )
    duty_people = duty_people[duty_people['ìƒë‹´ì—°ì›”'].notna()]

    duty_people_summary = duty_people.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬', values='ì•„ì´ë””_ì •ì œ')
    duty_people_summary = duty_people_summary.fillna(0).astype(int)
    duty_people_summary['í•©ê³„'] = duty_people_summary.sum(axis=1)
    duty_people_summary.loc['ëˆ„ê³„'] = duty_people_summary.sum()

    # 4. ì‹¤ê³„(ì•„ì´ë””ë³„ ëŒ€í‘œ ì§ê¸‰: 'ë¯¸ìƒ'ì´ ì•„ë‹Œ ê°’ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ)
    def get_representative_job(jobs):
        for job in jobs:
            if job != 'ë¯¸ìƒ':
                return job
        return 'ë¯¸ìƒ'

    id_job = (
        df_counseling
        .groupby('ì•„ì´ë””_ì •ì œ')['ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬']
        .apply(lambda jobs: get_representative_job(jobs))
        .reset_index()
    )

    real_by_duty = id_job.groupby('ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬')['ì•„ì´ë””_ì •ì œ'].nunique()
    real_duty_total = id_job['ì•„ì´ë””_ì •ì œ'].nunique()

    # 5. ì‹¤ê³„ í–‰ ë§Œë“¤ê¸°
    row_dict = {col: int(real_by_duty[col]) if col in real_by_duty and pd.notnull(real_by_duty[col]) else 0
                for col in duty_people_summary.columns if col != 'í•©ê³„'}
    row_dict['í•©ê³„'] = real_duty_total
    duty_people_summary.loc['ì‹¤ê³„'] = pd.Series(row_dict)

    # 6. ê²°ê³¼ ì¶œë ¥
    st.markdown("ì§ê¸‰ë³„ ì´ìš© ì¸ì›")
    st.dataframe(duty_people_summary)
    

    # --- ì§ê¸‰ë³„ ì´ìš© íšŸìˆ˜ (íšŒ) ---
    st.markdown("ì§ê¸‰ë³„ ì´ìš© íšŸìˆ˜ (íšŒ)")
    duty_count = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬'])['ì‚¬ë¡€ë²ˆí˜¸']
        .count()
        .reset_index()
    )
    duty_count_summary = duty_count.pivot(index='ìƒë‹´ì—°ì›”', columns='ì‹ ì²­ì§ì›ì§ë¬´_ì •ë¦¬', values='ì‚¬ë¡€ë²ˆí˜¸')
    duty_count_summary = duty_count_summary.fillna(0).astype(int)
    duty_count_summary['í•©ê³„'] = duty_count_summary.sum(axis=1)
    duty_count_summary.loc['ëˆ„ê³„'] = duty_count_summary.sum()

    st.dataframe(duty_count_summary)

    st.markdown("---")
    st.subheader("7) ìƒë‹´íšŒê¸°ë³„ ì¸ì› ë° íšŸìˆ˜")
    st.markdown("ìƒë‹´íšŒê¸°ë³„ ì´ìš© ì¸ì› (ëª…)")

    session_by_user_month = (
        df_counseling
        .dropna(subset=['ìƒë‹´ì—°ì›”', 'ì•„ì´ë””'])
        .groupby(['ìƒë‹´ì—°ì›”', 'ì•„ì´ë””'])
        .size()
        .reset_index(name='íšŒê¸°ìˆ˜')
    )

    months = sorted(session_by_user_month['ìƒë‹´ì—°ì›”'].unique())
    result = {}
    for m in months:
        sub = session_by_user_month[session_by_user_month['ìƒë‹´ì—°ì›”'] <= m]
        id_cum = sub.groupby('ì•„ì´ë””')['íšŒê¸°ìˆ˜'].sum()
        result[m] = id_cum

    # all_sessions: ì „ì²´ íšŒê¸°ìˆ˜(1,2,...)
    all_sessions = set()
    for ser in result.values():
        all_sessions.update(ser.values)
    all_sessions = sorted([int(x) for x in all_sessions if pd.notnull(x)])

    # ì›”ë³„ í‘œ ë§Œë“¤ê¸°
    table = pd.DataFrame(index=all_sessions, columns=months)
    for m in months:
        id_cum = result[m]
        for n in all_sessions:
            prev = result[months[months.index(m)-1]] if months.index(m) > 0 else pd.Series([], dtype=int)
            ids_n = set(id_cum[id_cum == n].index)
            if months.index(m) > 0:
                ids_prev = set(prev[prev >= n].index)
                ids_n = ids_n - ids_prev
            table.loc[n, m] = len(ids_n)

    # í•©ê³„ í–‰(ë§¨ ì•„ë˜)
    table.loc['í•©ê³„', months] = [
        session_by_user_month[session_by_user_month['ìƒë‹´ì—°ì›”'] == m]['ì•„ì´ë””'].nunique() for m in months
    ]

    # í•©ê³„ ì—´(ë§¨ ì˜¤ë¥¸ìª½): ì „ì²´ ê¸°ê°„ ëˆ„ì  íšŒê¸°ìˆ˜ë³„ ìµœì¢… ì¸ì›ìˆ˜
    # â‘  ë¨¼ì € ê° ì•„ì´ë””ê°€ ì „ì²´ ê¸°ê°„ ë™ì•ˆ ë°›ì€ ì´ íšŒê¸°ìˆ˜ ì§‘ê³„
    user_total_sessions = session_by_user_month.groupby('ì•„ì´ë””')['íšŒê¸°ìˆ˜'].sum()

    # â‘¡ ê° íšŒê¸°ìˆ˜(n)ë³„ë¡œ ë§ˆì§€ë§‰ì´ níšŒê¸°ì¸ ê³ ìœ  ì¸ì›ìˆ˜ë¥¼ ì…ˆ
    table['í•©ê³„'] = [(user_total_sessions == n).sum() for n in all_sessions] + [user_total_sessions.shape[0]]

    # â‘¢ ë§¨ ì•„ë˜ 'í•©ê³„' ì…€ì—ëŠ” ì „ì²´ ê³ ìœ  ì¸ì›ìˆ˜
    table.loc['í•©ê³„', 'í•©ê³„'] = user_total_sessions.shape[0]

    # (ë§ˆì§€ë§‰ ë§ˆë¬´ë¦¬: NaN â†’ 0)
    table = table.fillna(0).astype(int)

    st.dataframe(table)

    # 1. ìƒë‹´ì¼ ì˜¤ë¦„ì°¨ìˆœ, ì•„ì´ë””ë³„ ì •ë ¬
    df_counseling = df_counseling.sort_values(['ì•„ì´ë””', 'ìƒë‹´ì‹¤ì‹œì¼'])

    # 2. ìƒë‹´ì—°ì›” ëª©ë¡
    months = sorted(df_counseling['ìƒë‹´ì—°ì›”'].dropna().unique())

    # 3. ë‚´ë‹´ìë³„ ëˆ„ì  íšŒê¸° ë¶€ì—¬
    df_counseling['ëˆ„ì íšŒê¸°'] = df_counseling.groupby('ì•„ì´ë””').cumcount() + 1

    # 4. ì›”ë³„ë¡œ ëˆ„ì íšŒê¸°ë³„ ê±´ìˆ˜ ì§‘ê³„
    session_count_table = pd.pivot_table(
        df_counseling,
        index='ëˆ„ì íšŒê¸°',
        columns='ìƒë‹´ì—°ì›”',
        values='ì•„ì´ë””',
        aggfunc='count',
        fill_value=0
    )

    # 5. í•©ê³„ì—´/í–‰ ì¶”ê°€
    session_count_table['í•©ê³„'] = session_count_table.sum(axis=1)
    session_count_table.loc['í•©ê³„'] = session_count_table.sum(axis=0)

    st.markdown("ìƒë‹´íšŒê¸°ë³„ ì‹¤ì œ ì´ìš© ê±´ìˆ˜")
    st.dataframe(session_count_table)


    st.markdown("---")
    st.header("ë‚´ë‹´ìë³„ ì „ì²´ ìƒë‹´ ì´ìš© íšŸìˆ˜")
    client_counts = df_counseling.groupby('ì•„ì´ë””')['ì‚¬ë¡€ë²ˆí˜¸'].count().reset_index()
    client_counts.columns = ['ì•„ì´ë””', 'ìƒë‹´íšŸìˆ˜']
    client_counts['ì•„ì´ë””'] = client_counts['ì•„ì´ë””'].apply(lambda x: str(int(float(x))) if pd.notnull(x) and str(x).replace('.', '', 1).isdigit() else str(x))
    client_counts.index = client_counts.index + 1
    client_counts.reset_index(inplace=True)
    client_counts.rename(columns={'index': 'No'}, inplace=True)

    total_row = pd.DataFrame([{
        'No': 'í•©ê³„',
        'ì•„ì´ë””': f"ì´ {client_counts['ì•„ì´ë””'].nunique()}ëª…",
        'ìƒë‹´íšŸìˆ˜': client_counts['ìƒë‹´íšŸìˆ˜'].sum()
    }])
    client_counts_with_total = pd.concat([client_counts, total_row], ignore_index=True)
    st.dataframe(client_counts_with_total, use_container_width=True)

    # ------------------------------
    # [ì—‘ì…€ ë‹¤ìš´ë¡œë“œìš©] ìš”ì•½í‘œë“¤ì„ dictì— ì €ì¥
    # ------------------------------
    excel_tables = {
        "ìƒë‹´ìœ í˜•ë³„_ì´ìš©ì¸ì›": type_people_summary,
        "ìƒë‹´ìœ í˜•ë³„_ì´ìš©íšŸìˆ˜": type_counts_summary,
        "ì‹ ì²­ìœ í˜•ë³„_ì´ìš©ì¸ì›": ì‹ ì²­ìœ í˜•_people_summary,
        "ì‹ ì²­ìœ í˜•ë³„_ì´ìš©íšŸìˆ˜": ì‹ ì²­ìœ í˜•_counts_summary,
        "ì„±ë³„_ì´ìš©ì¸ì›": gender_people_summary,
        "ì„±ë³„_ì´ìš©íšŸìˆ˜": gender_counts_summary,
        "ì—°ë ¹ë³„_ì´ìš©ì¸ì›": age_pivot_people,
        "ì—°ë ¹ë³„_ì´ìš©íšŸìˆ˜": age_pivot_cases,
        "ì†Œì†ë³„_ì´ìš©ì¸ì›": ì†Œì†_people_summary,
        "ì†Œì†ë³„_ì´ìš©íšŸìˆ˜": ì†Œì†_counts_summary,
        "ì§ê¸‰ë³„_ì´ìš©ì¸ì›": duty_people_summary,
        "ì§ê¸‰ë³„_ì´ìš©íšŸìˆ˜": duty_count_summary,
        "ìƒë‹´íšŒê¸°ë³„_ì´ìš©ì¸ì›": table,
        "ìƒë‹´íšŒê¸°ë³„_ì´ìš©íšŸìˆ˜": session_count_table,
        "ë‚´ë‹´ìë³„_ì „ì²´ìƒë‹´íšŸìˆ˜": client_counts_with_total,
    }

    # ------------------------------
    # [ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼] 
    # ------------------------------
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for sheet_name, table in excel_tables.items():
            # ì¸ë±ìŠ¤ê°€ ì˜ë¯¸ìˆëŠ” í…Œì´ë¸”ë§Œ index=True (ë³´í†µ False ì¶”ì²œ)
            table.to_excel(writer, sheet_name=sheet_name, index=True if table.index.name else False)
    output.seek(0)
    st.download_button(
        label="ëª¨ë“  ì§‘ê³„í‘œ í†µí•© ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=output,
        file_name="ìƒë‹´_ìš”ì•½_í†µê³„í‘œ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # ì‹¬ë¦¬ì§„ë‹¨ ì´ìš© ì¸ì› ë° íšŸìˆ˜
elif menu == "ğŸ§  ì‹¬ë¦¬ì§„ë‹¨ í†µê³„":
    if 'df_diagnosis' not in st.session_state or 'all_months' not in st.session_state:
        st.warning("ë¨¼ì € 'íŒŒì¼ ì—…ë¡œë“œ'ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()
    df_diagnosis = st.session_state['df_diagnosis']
    all_months = st.session_state['all_months']

    st.header("ğŸ§  ì‹¬ë¦¬ì§„ë‹¨ ì´ìš© ì¸ì› ë° íšŸìˆ˜")
    diag_people = df_diagnosis.groupby(['ì§„ë‹¨ì—°ì›”', 'ì§„ë‹¨ëª…'])['ì•„ì´ë””'].nunique().reset_index()
    diag_people_summary = diag_people.pivot(index='ì§„ë‹¨ì—°ì›”', columns='ì§„ë‹¨ëª…', values='ì•„ì´ë””').fillna(0).astype(int)

    real_monthly_people = (
        df_diagnosis.dropna(subset=['ì§„ë‹¨ì—°ì›”', 'ì•„ì´ë””'])
        .groupby('ì§„ë‹¨ì—°ì›”')['ì•„ì´ë””'].nunique()
    )

    diag_people_summary['í•©ê³„'] = diag_people_summary.index.map(real_monthly_people)
    diag_people_summary.loc['ëˆ„ê³„'] = diag_people_summary.sum()

    def clean_id(val):
        if pd.isnull(val):
            return None
        val = str(val).strip().replace(' ', '').replace('\u3000', '').lower()
        if val.endswith('.0'):
            val = val[:-2]
        if not val or val in {'nan', 'none', '0', '0.0'}:
            return None
        return val

    real_by_test = (
        df_diagnosis.dropna(subset=['ì§„ë‹¨ëª…', 'ì•„ì´ë””'])
        .assign(ì•„ì´ë””=lambda x: x['ì•„ì´ë””'].apply(clean_id))
        .dropna(subset=['ì•„ì´ë””'])
        .drop_duplicates(['ì§„ë‹¨ëª…', 'ì•„ì´ë””'])
        .groupby('ì§„ë‹¨ëª…')['ì•„ì´ë””'].nunique()
    )
    real_people_count = (
        df_diagnosis['ì•„ì´ë””'].apply(clean_id).dropna().drop_duplicates().shape[0]
    )

    row_dict = {col: int(real_by_test[col]) if col in real_by_test and pd.notnull(real_by_test[col]) else 0 for col in diag_people_summary.columns}
    row_dict['í•©ê³„'] = real_people_count
    diag_people_summary.loc['ì‹¤ê³„'] = row_dict

    st.markdown("ì‹¬ë¦¬ì§„ë‹¨ ì´ìš© ì¸ì›")
    st.dataframe(diag_people_summary)

    diag_counts = df_diagnosis.groupby(['ì§„ë‹¨ì—°ì›”', 'ì§„ë‹¨ëª…'])['ì‹œí–‰ë²ˆí˜¸'].count().reset_index()
    diag_counts_summary = diag_counts.pivot(index='ì§„ë‹¨ì—°ì›”', columns='ì§„ë‹¨ëª…', values='ì‹œí–‰ë²ˆí˜¸').fillna(0).astype(int)
    diag_counts_summary['í•©ê³„'] = diag_counts_summary.sum(axis=1)
    diag_counts_summary.loc['ëˆ„ê³„'] = diag_counts_summary.sum()
    st.markdown("ì‹¬ë¦¬ì§„ë‹¨ ì´ìš© íšŸìˆ˜")
    st.dataframe(diag_counts_summary)
    st.markdown("---")

elif menu == "ğŸ—‚ï¸ ìƒë‹´ ì£¼ì œë³„ í†µê³„":
    if 'df_counseling' not in st.session_state:
        st.warning("ë¨¼ì € 'íŒŒì¼ ì—…ë¡œë“œ'ì—ì„œ ìƒë‹´ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()
    df_counseling = st.session_state['df_counseling']

# --- ì§‘ê³„ í•¨ìˆ˜ ì •ì˜ ---
def make_topic_stats_with_area(df, main_col, sub_col, header_text):
    st.markdown(f"#### {header_text}")
    count_df = (
        df.groupby(['ì˜ì—­', main_col, sub_col])
        .size().reset_index(name='ìƒë‹´ê±´ìˆ˜')
        .sort_values(['ì˜ì—­', main_col, sub_col])
        .reset_index(drop=True)
    )
    st.dataframe(count_df)
    # ê²°ì¸¡ì¹˜ ì•ˆë‚´
    missing_main = df[main_col].isnull().sum()
    missing_sub = df[sub_col].isnull().sum()
    if missing_main > 0 or missing_sub > 0:
        st.warning(f"'{main_col}' ë˜ëŠ” '{sub_col}' ì—´ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ë¶„ì„ì—ì„œ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        with st.expander(f"{main_col} ë˜ëŠ” {sub_col} ê²°ì¸¡ì¹˜ í–‰ ë³´ê¸°"):
            st.dataframe(df[df[main_col].isnull() | df[sub_col].isnull()][['ì‚¬ë¡€ë²ˆí˜¸', 'ì•„ì´ë””', 'ìƒë‹´ì‹¤ì‹œì¼', 'ì˜ì—­', main_col, sub_col]])

# --- ì‹¤ì œ ì§‘ê³„ í‘œ ì¶œë ¥ ---
if menu == "ğŸ—‚ï¸ ìƒë‹´ ì£¼ì œë³„ í†µê³„":
    st.header("ìƒë‹´ ì£¼ì œë³„ í†µê³„ (ì˜ì—­ í¬í•¨)")
    make_topic_stats_with_area(df_counseling.rename(columns={'ì˜ì—­1': 'ì˜ì—­'}), 'ì£¼í˜¸ì†Œ1', 'í•˜ìœ„ìš”ì†Œ1', "1) ì˜ì—­ Â· ì£¼í˜¸ì†Œ1 Â· í•˜ìœ„ìš”ì†Œ1")
    make_area_sum_table(df_counseling, 'ì˜ì—­1', 'ì£¼í˜¸ì†Œ1', 'í•˜ìœ„ìš”ì†Œ1', label="ì£¼í˜¸ì†Œ1")
    make_main_issue_sum_table(df_counseling, 'ì˜ì—­1', 'ì£¼í˜¸ì†Œ1')

    make_topic_stats_with_area(df_counseling.rename(columns={'ì˜ì—­2': 'ì˜ì—­'}), 'ì£¼í˜¸ì†Œ2', 'í•˜ìœ„ìš”ì†Œ2', "2) ì˜ì—­ Â· ì£¼í˜¸ì†Œ2 Â· í•˜ìœ„ìš”ì†Œ2")
    make_area_sum_table(df_counseling, 'ì˜ì—­2', 'ì£¼í˜¸ì†Œ2', 'í•˜ìœ„ìš”ì†Œ2', label="ì£¼í˜¸ì†Œ2")
    make_main_issue_sum_table(df_counseling, 'ì˜ì—­2', 'ì£¼í˜¸ì†Œ2')

    make_topic_stats_with_area(df_counseling.rename(columns={'ì˜ì—­3': 'ì˜ì—­'}), 'ì£¼í˜¸ì†Œ3', 'í•˜ìœ„ìš”ì†Œ3', "3) ì˜ì—­ Â· ì£¼í˜¸ì†Œ3 Â· í•˜ìœ„ìš”ì†Œ3")
    make_area_sum_table(df_counseling, 'ì˜ì—­3', 'ì£¼í˜¸ì†Œ3', 'í•˜ìœ„ìš”ì†Œ3', label="ì£¼í˜¸ì†Œ3")
    make_main_issue_sum_table(df_counseling, 'ì˜ì—­3', 'ì£¼í˜¸ì†Œ3')